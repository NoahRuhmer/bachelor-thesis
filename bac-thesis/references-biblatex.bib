@article{bellman1957markovian,
	title={A Markovian decision process},
	author={Bellman, Richard},
	journal={Journal of mathematics and mechanics},
	pages={679--684},
	year={1957},
	publisher={JSTOR}
}

@article{bellman1966dynamic,
	title={Dynamic programming},
	author={Bellman, Richard},
	journal={Science},
	volume={153},
	number={3731},
	pages={34--37},
	year={1966},
	publisher={American Association for the Advancement of Science}
}
@book{ross2014introduction,
title={Introduction to stochastic dynamic programming},
author={Ross, Sheldon M},
year={2014},
publisher={Academic press}
}
@book{puterman2014markov,
	title={Markov decision processes: discrete stochastic dynamic programming},
	author={Puterman, Martin L},
	year={2014},
	publisher={John Wiley \& Sons}
}

@article{watkins1989learning,
	title={Learning from delayed rewards},
	author={Watkins, Christopher John Cornish Hellaby},
	year={1989},
	publisher={King's College, Cambridge United Kingdom}
}
@article{watkins1992q,
	title={Q-learning},
	author={Watkins, Christopher JCH and Dayan, Peter},
	journal={Machine learning},
	volume={8},
	number={3},
	pages={279--292},
	year={1992},
	publisher={Springer}
}

@book{sutton2018reinforcement,
	title={Reinforcement learning: An introduction},
	author={Sutton, Richard S and Barto, Andrew G},
	year={2018},
	publisher={MIT press}
}

@book{bertsekas2019reinforcement,
	title={Reinforcement learning and optimal control},
	author={Bertsekas, Dimitri},
	year={2019},
	publisher={Athena Scientific}
}
@book{bertsekas2012dynamic,
	title={Dynamic programming and optimal control: Volume I},
	author={Bertsekas, Dimitri},
	volume={1},
	year={2012},
	publisher={Athena scientific}
}

@article{ZHANG202040,
	title = {Deterministic policy gradient adaptive dynamic programming for model-free optimal control},
	journal = {Neurocomputing},
	volume = {387},
	pages = {40-50},
	year = {2020},
	issn = {0925-2312},
	doi = {https://doi.org/10.1016/j.neucom.2019.11.032},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231219316303},
	author = {Yongwei Zhang and Bo Zhao and Derong Liu},
	keywords = {Adaptive dynamic programming, Deterministic policy gradient, Optimal control, Neural networks, Model-free},
	abstract = {In this paper, a deterministic policy gradient adaptive dynamic programming (DPGADP) algorithm is proposed for solving model-free optimal control problems of discrete-time nonlinear systems. By using the measured data, the developed algorithm improves the control performance with the policy gradient method. The convergence of DPGADP algorithm is demonstrated by showing that the constructed Q-function sequence is monotonically non-increasing and converges to the optimal Q-function. An actor-critic neural network (NN) structure is established to implement the DPGADP algorithm. Experience replay and target network techniques from deep Q-learning are employed during the training process. The stability of the NN weight error dynamics is also analyzed. Finally, two simulation examples are presented to verify the effectiveness of the proposed method.}
}


@book{bertsekas2022abstract,
	title={Abstract dynamic programming},
	author={Bertsekas, Dimitri},
	year={2022},
	publisher={Athena Scientific}
}

@book{busoniu2017reinforcement,
	title={Reinforcement learning and dynamic programming using function approximators},
	author={Busoniu, Lucian and Babuska, Robert and De Schutter, Bart and Ernst, Damien},
	year={2017},
	publisher={CRC press}
}


@book{powell2007approximate,
	title={Approximate Dynamic Programming: Solving the curses of dimensionality},
	author={Powell, Warren B},
	volume={703},
	year={2007},
	publisher={John Wiley \& Sons}
}

@article{bertsekas2008approximate,
	title={Approximate dynamic programming},
	author={Bertsekas, Dimitri P},
	year={2008},
	publisher={Citeseer}
}

@book{si2004handbook,
	title={Handbook of learning and approximate dynamic programming},
	author={Si, Jennie and Barto, Andrew G and Powell, Warren B and Wunsch, Don},
	volume={2},
	year={2004},
	publisher={John Wiley \& Sons}
}

@inproceedings{gu2016continuous,
	title={Continuous deep q-learning with model-based acceleration},
	author={Gu, Shixiang and Lillicrap, Timothy and Sutskever, Ilya and Levine, Sergey},
	booktitle={International conference on machine learning},
	pages={2829--2838},
	year={2016},
	organization={PMLR}
}
